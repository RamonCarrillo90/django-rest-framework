"""
Script para convertir el modelo Keras a TFLite
Esto reduce el uso de RAM de ~2GB a ~200MB
"""
import tensorflow as tf
import os

# Rutas
KERAS_MODEL_PATH = "api/ml/best_model_sin_patron_ceros.keras"
TFLITE_MODEL_PATH = "api/ml/modelo.tflite"

def convert_keras_to_tflite():
    """Convierte el modelo Keras a TFLite"""

    print("=" * 60)
    print("CONVERSI√ìN DE MODELO KERAS A TFLITE")
    print("=" * 60)

    # Verificar que el modelo Keras existe
    if not os.path.exists(KERAS_MODEL_PATH):
        print(f"‚ùå ERROR: No se encontr√≥ el modelo en {KERAS_MODEL_PATH}")
        return False

    print(f"\nüìÇ Cargando modelo Keras desde: {KERAS_MODEL_PATH}")

    try:
        # Cargar modelo Keras
        model = tf.keras.models.load_model(KERAS_MODEL_PATH)
        print("‚úÖ Modelo Keras cargado exitosamente")

        # Mostrar informaci√≥n del modelo
        print(f"\nüìä Informaci√≥n del modelo:")
        print(f"   - Input shape: {model.input_shape}")
        print(f"   - Output shape: {model.output_shape}")
        print(f"   - N√∫mero de par√°metros: {model.count_params():,}")

        # Convertir a TFLite
        print(f"\nüîÑ Convirtiendo a TFLite...")
        converter = tf.lite.TFLiteConverter.from_keras_model(model)

        # Optimizaciones opcionales (descomenta si quieres modelo m√°s peque√±o)
        # converter.optimizations = [tf.lite.Optimize.DEFAULT]

        tflite_model = converter.convert()

        # Guardar modelo TFLite
        print(f"üíæ Guardando modelo TFLite en: {TFLITE_MODEL_PATH}")
        with open(TFLITE_MODEL_PATH, 'wb') as f:
            f.write(tflite_model)

        # Mostrar tama√±os
        keras_size = os.path.getsize(KERAS_MODEL_PATH) / (1024 * 1024)
        tflite_size = os.path.getsize(TFLITE_MODEL_PATH) / (1024 * 1024)

        print(f"\nüìè Tama√±os de archivo:")
        print(f"   - Modelo Keras:  {keras_size:.2f} MB")
        print(f"   - Modelo TFLite: {tflite_size:.2f} MB")
        print(f"   - Reducci√≥n:     {((keras_size - tflite_size) / keras_size * 100):.1f}%")

        print("\n" + "=" * 60)
        print("‚úÖ CONVERSI√ìN EXITOSA")
        print("=" * 60)
        print("\nPr√≥ximos pasos:")
        print("1. Verifica que el archivo 'modelo.tflite' fue creado")
        print("2. Haz commit del nuevo archivo:")
        print("   git add api/ml/modelo.tflite")
        print("   git commit -m 'Add TFLite model for production'")
        print("3. Actualiza requirements.txt (quita tensorflow)")
        print("4. Despliega en Render")
        print()

        return True

    except Exception as e:
        print(f"\n‚ùå ERROR durante la conversi√≥n: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_tflite_model():
    """Prueba que el modelo TFLite funciona correctamente"""

    print("\n" + "=" * 60)
    print("PROBANDO MODELO TFLITE")
    print("=" * 60)

    if not os.path.exists(TFLITE_MODEL_PATH):
        print(f"‚ùå ERROR: No se encontr√≥ el modelo TFLite en {TFLITE_MODEL_PATH}")
        return False

    try:
        import numpy as np
        import tflite_runtime.interpreter as tflite

        # Cargar int√©rprete TFLite
        print(f"\nüìÇ Cargando modelo TFLite...")
        interpreter = tflite.Interpreter(model_path=TFLITE_MODEL_PATH)
        interpreter.allocate_tensors()

        # Obtener detalles de entrada/salida
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()

        print("‚úÖ Modelo TFLite cargado exitosamente")
        print(f"\nüìä Detalles del modelo:")
        print(f"   - Input shape:  {input_details[0]['shape']}")
        print(f"   - Output shape: {output_details[0]['shape']}")

        # Crear datos de prueba
        input_shape = input_details[0]['shape']
        test_data = np.random.random(input_shape).astype(np.float32)

        # Hacer predicci√≥n de prueba
        print(f"\nüîÆ Haciendo predicci√≥n de prueba...")
        interpreter.set_tensor(input_details[0]['index'], test_data)
        interpreter.invoke()
        output_data = interpreter.get_tensor(output_details[0]['index'])

        print(f"‚úÖ Predicci√≥n exitosa")
        print(f"   - Shape de salida: {output_data.shape}")
        print(f"   - Suma de probabilidades: {np.sum(output_data):.4f}")

        print("\n" + "=" * 60)
        print("‚úÖ PRUEBA EXITOSA - El modelo TFLite funciona correctamente")
        print("=" * 60)

        return True

    except ImportError:
        print("\n‚ö†Ô∏è  ADVERTENCIA: No se pudo importar tflite_runtime")
        print("   Esto es normal si no lo tienes instalado localmente.")
        print("   El modelo funcionar√° en producci√≥n.")
        return True

    except Exception as e:
        print(f"\n‚ùå ERROR durante la prueba: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("\nüöÄ Iniciando conversi√≥n de modelo Keras a TFLite\n")

    # Convertir modelo
    success = convert_keras_to_tflite()

    if success:
        # Probar modelo
        test_tflite_model()
    else:
        print("\n‚ùå La conversi√≥n fall√≥. Revisa los errores arriba.")
        exit(1)
